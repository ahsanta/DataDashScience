{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec357b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import yfinance as yf \n",
    "from itertools import product \n",
    "import talib as ta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "plt.style.use('seaborn')\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_conf_mat(cm, classes, title, cmap = plt.cm.Blues):                                 \n",
    "    #Plot confusion matrix to see True Positives, False Positives, True Negatives and False Negatives \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    #why cm.max()  /2??\n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"green\" if cm[i, j] > thresh else \"red\", fontsize = 20)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "        \n",
    "def model_evaluation(y_pred, y_test):\n",
    "    c_mat = confusion_matrix(y_test, y_pred, normalize = 'all') ## Predicted vs. actual outcome\n",
    "    auc = round(roc_auc_score(y_test, y_pred),4)\n",
    "    accuracy = round(accuracy_score(y_test,y_pred) ,4)\n",
    "    recall = round(recall_score(y_test, y_pred),4)\n",
    "    precision = round(precision_score(y_test, y_pred),4)\n",
    "    class_names = ['Positive', 'Negative'] ## Different class names\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    plot_conf_mat(c_mat,classes=class_names,title=\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Area under the curve (AUC): {auc}\")\n",
    "    print(f\"Recall Score: {recall}\")\n",
    "    auc = round(roc_auc_score(y_test, y_pred), 4)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, 'b',\n",
    "    label='AUC = %0.2f'% auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'r--')\n",
    "    plt.xlim([-0.1,1.1])\n",
    "    plt.ylim([-0.1,1.1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25907255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
